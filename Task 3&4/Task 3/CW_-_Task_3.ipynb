{"cells":[{"cell_type":"markdown","id":"383de715","metadata":{},"source":["Coursework - Task 3"]},{"cell_type":"code","execution_count":2,"id":"82bc79fd","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(60000, 785)\n","   label     0\n","0      0  6000\n","1      1  6000\n","2      2  6000\n","3      3  6000\n","4      4  6000\n","5      5  6000\n","6      6  6000\n","7      7  6000\n","8      8  6000\n","9      9  6000\n","(784, 60000)\n","(10, 60000)\n"]}],"source":["# Importing the Librarys\n","import pandas as pd\n","import numpy as np\n","\n","\n","# Importing and reading the data from the csv file\n","np.random.seed(42)\n","data = pd.read_csv('fashion-mnist_train.csv')\n","print(data.shape)\n","data = data.sample(frac=1)\n","print(data[['label']].groupby('label').size().reset_index())\n","\n","# Converting the clothes categories to one-hot encoded vectors and prepares the training and testing\n","one_hot = pd.get_dummies(data['label'].unique())\n","one_hot['label'] = one_hot.index\n","\n","data = pd.merge(data,one_hot)\n","data = data.sample(frac=1)\n","data_train = data\n","data_test = pd.read_csv('fashion-mnist_test.csv')\n","data_test = pd.merge(data_test,one_hot)\n","data_train.drop('label',axis=1,inplace=True)\n","data_test.drop('label',axis=1,inplace=True)\n","\n","# Creating the training and testing set\n","X_train = np.array(data_train.drop([0,1,2,3,4,5,6,7,8,9],axis=1).values)/255\n","y_train = np.array(data_train[[0,1,2,3,4,5,6,7,8,9]].values)\n","X_test = np.array(data_test.drop([0,1,2,3,4,5,6,7,8,9],axis=1).values)/255\n","y_test = np.array(data_test[[0,1,2,3,4,5,6,7,8,9]].values)\n","\n","X_train = X_train.T\n","y_train = y_train.T\n","print(X_train.shape)\n","print(y_train.shape)\n","X_test = X_test.T\n","y_test = y_test.T"]},{"cell_type":"markdown","id":"aab8c25a","metadata":{},"source":["Implementing Sigmoid, Relu and Softmax layers"]},{"cell_type":"code","execution_count":3,"id":"abad64d3","metadata":{},"outputs":[],"source":["# Initializing the Weight Matrices\n","def sigmoid(x):\n","    y = 1./(1+np.exp(-x))\n","    return y\n","\n","def sigmoid_backpass(x):\n","    s = 1/(1+np.exp(-x))\n","    dx = s * (1-s)\n","    return dx\n","\n","def relu(x):\n","    y = np.maximum(0,x)\n","    return y\n","\n","def relu_backpass(x):\n","    x[x<=0] = 0\n","    x[x>0] = 1\n","    return x\n","\n","def softmax(x):\n","    e_x = np.exp(x - np.max(x))\n","    total = e_x.sum(axis=0)\n","    return (e_x / total) "]},{"cell_type":"markdown","id":"2f0d5c59","metadata":{},"source":["â€¢\tImplement an optimizer (e.g. Adam) "]},{"cell_type":"code","execution_count":4,"id":"59b7aae8","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'nn' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/var/folders/1x/151x284x3hv6jvqsv5z9bttc0000gn/T/ipykernel_10473/2115661457.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}],"source":["# Optimizer\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(),lr= 0.1)"]},{"cell_type":"markdown","id":"666fe0e8","metadata":{},"source":["Implementing a fully parameterizable network"]},{"cell_type":"code","execution_count":5,"id":"aea64c6f","metadata":{},"outputs":[],"source":["# Initialize a network\n","def initialize_network(inputs, hidden, outputs):\n","    network = list()\n","    #Define hidden layers of the network. Weights are randomised\n","    hidden_layer = [{'weights':[random() for i in range(inputs + 1)]} for i in range(hidden)]\n","    #Add the defined hidden layer to the network. More layers can easily be added\n","    network.append(hidden_layer)\n","    #Define the output layer of the network. Weights are randomised\n","    output_layer = [{'weights':[random() for i in range(hidden + 1)]} for i in range(outputs)]\n","    #Add the defined hidden layer to the network.\n","    network.append(output_layer)\n","    return network\n"," \n","# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","    activation = 0\n","    # Neuron activation is calculated as the weighted sum of inputs\n","    for i in range(len(weights)-1):\n","        activation += weights[i] * inputs[i]\n","    #Still have to include the weight that has now input to multiply with\n","    activation += weights[-1]\n","    return activation\n","\n","# Return the derivative of an output\n","def transfer_derivative(x):\n","    y = x * (1.0 - x)\n","    return y\n","\n","# Forward propagate inputs to a network output\n","def forward_propagate(network, row):\n","    inputs = row\n","    for layer in network:\n","        new_inputs = []\n","        #For every neuron in a layer, calculate the activation and active it with an activation function to get new inputs\n","        for neuron in layer:\n","            activation = activate(neuron['weights'], inputs)\n","            neuron['output'] = sigmoid(activation)\n","            #neuron['output'] = relu(activation)\n","            #neuron['output'] = softmax(activation)\n","            new_inputs.append(neuron['output'])\n","        # replace old input with new input for the next layer in the network\n","        inputs = new_inputs\n","    return inputs"]},{"cell_type":"markdown","id":"1d067310","metadata":{},"source":["Training using Backpropagation"]},{"cell_type":"code","execution_count":7,"id":"3b571d19","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test:  0\n","Testing Accuracy: 0.0781\n","Training Accuracy: 0.1\n","\n","\n","Test:  1\n","Testing Accuracy: 0.3341\n","Training Accuracy: 0.23521666666666666\n","\n","\n","Test:  2\n","Testing Accuracy: 0.2557\n","Training Accuracy: 0.25983333333333336\n","\n","\n","Test:  3\n","Testing Accuracy: 0.2697\n","Training Accuracy: 0.18738333333333335\n","\n","\n","Test:  4\n","Testing Accuracy: 0.2229\n","Training Accuracy: 0.4633\n","\n","\n","Test:  5\n","Testing Accuracy: 0.2464\n","Training Accuracy: 0.37006666666666665\n","\n","\n","Test:  6\n","Testing Accuracy: 0.3217\n","Training Accuracy: 0.07195\n","\n","\n","Test:  7\n","Testing Accuracy: 0.1401\n","Training Accuracy: 0.13668333333333332\n","\n","\n","Test:  8\n","Testing Accuracy: 0.32\n","Training Accuracy: 0.6085\n","\n","\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/var/folders/1x/151x284x3hv6jvqsv5z9bttc0000gn/T/ipykernel_10473/2114954289.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpermutation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mX_train_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mY_train_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import random\n","random.seed(42)\n","w1 = np.random.rand(128,784)/np.sqrt(784)\n","b0 = np.zeros((128,1))/np.sqrt(784)\n","w2 = np.random.rand(10,128)/np.sqrt(128)\n","b1 = np.zeros((10,1))/np.sqrt(128)\n","loss=[]\n","batches = 1000\n","\n","lr = 0.1\n","batch_size = 200\n","beta = 0.9\n","count = 0\n","epochs = 500\n","\n","loss_weight_dict = {\n","\n","}\n","\n","# For every one cycle of training the neural network\n","for i in range(epochs):\n","    permutation = np.random.permutation(X_train.shape[1])\n","    X_train_shuffled = X_train[:, permutation]\n","    Y_train_shuffled = y_train[:, permutation]\n","    \n","    for j in range(batches):\n","        \n","        begin = j * batch_size\n","        end = min(begin + batch_size, X_train.shape[1] - 1)\n","        if begin>end:\n","            continue\n","        #optimizer.zero_grad()\n","        X = X_train_shuffled[:, begin:end]\n","        Y = Y_train_shuffled[:, begin:end]\n","        m_batch = end - begin\n","        x1 = sigmoid(w1@X+b0)\n","        x2 = softmax(w2@x1+b1)\n","\n","        delta_2 = (x2-Y)\n","        delta_1 = np.multiply(w2.T@delta_2, np.multiply(x1,1-x1))\n","        if i==0 :\n","            dW1 = delta_1@X.T\n","            dW2 = delta_2@x1.T\n","            db0 = np.sum(delta_1,axis=1,keepdims=True)\n","            db1 = np.sum(delta_2,axis=1,keepdims=True)\n","        else:\n","            dW1_old = dW1\n","            dW2_old = dW2\n","            db0_old = db0\n","            db1_old = db1\n","            dW1 = delta_1@X.T\n","            dW2 = delta_2@x1.T\n","            db0 = np.sum(delta_1,axis=1,keepdims=True)\n","            db1 = np.sum(delta_2,axis=1,keepdims=True)\n","            \n","            # Using the past gradients to calculate the present gradients\n","            dW1 = (beta * dW1_old + (1. - beta) * dW1)\n","            db0 = (beta * db0_old + (1. - beta) * db0)\n","            dW2 = (beta * dW2_old + (1. - beta) * dW2)\n","            db1 = (beta * db1_old + (1. - beta) * db1)\n","\n","\n","        w1 = w1 - (1./m_batch)*(dW1)*lr\n","        b0 = b0 - (1./m_batch)*(db0)*(lr)\n","        w2 = w2 - (1./m_batch)*(dW2)*lr\n","        b1 = b1 - (1./m_batch)*(db1)*(lr)\n","    \n","    #optimizer.step()\n","\n","    x1 = sigmoid(w1@X_train+b0)\n","    x2_train = softmax(w2@x1+b1)\n","    x2_train_df = pd.DataFrame(x2_train)\n","    x2_train_df = (x2_train_df == x2_train_df.max()).astype(int)\n","    x2_train_df = x2_train_df.transpose()\n","    x2_train_df = pd.merge(x2_train_df,one_hot)\n","    x2_train_df = x2_train_df[['label']]\n","    y_train_df = pd.merge(pd.DataFrame(y_train.T),one_hot)\n","    x2_train_df['label_actual'] = y_train_df['label']\n","    train_accuracy = np.sum(x2_train_df['label_actual']==x2_train_df['label'])/x2_train_df.shape[0]\n","    add_loss = {\n","        'loss' : -np.mean(np.multiply(y_train,np.log(x2_train))),\n","        'weight_1' : w1,\n","        'weight_2':w2,\n","        'b0' : b0,\n","        'b1': b1,\n","        'train_accuracy': train_accuracy\n","    }\n","     \n","    x1 = sigmoid(w1@X_test+b0)\n","    x2_test = softmax(w2@x1+b1)\n","    x2_test_df = pd.DataFrame(x2_test)\n","    x2_test_df = (x2_test_df == x2_test_df.max()).astype(int)\n","    x2_test_df = x2_test_df.transpose()\n","    x2_test_df = pd.merge(x2_test_df,one_hot)\n","    x2_test_df = x2_test_df[['label']]\n","    y_test_df = pd.merge(pd.DataFrame(y_test.T),one_hot)\n","    x2_test_df['label_actual'] = y_test_df['label']\n","    test_accuracy = np.sum(x2_test_df['label_actual']==x2_test_df['label'])/x2_test_df.shape[0]\n","    print('Test: ',i)\n","\n","    # Print Accuracy\n","    print('Testing Accuracy:',test_accuracy)\n","    print('Training Accuracy:',train_accuracy)\n","    print('\\n')\n","    \n","    add_loss['testing_loss'] = -np.mean(np.multiply(y_test,np.log(x2_test)))\n","    add_loss['test_accuracy'] = test_accuracy\n","    loss_weight_dict[count] = add_loss\n","    count = count + 1\n","    "]},{"cell_type":"code","execution_count":null,"id":"c79d303c","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":5}
